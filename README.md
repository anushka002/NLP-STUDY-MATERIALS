
## Important Links-

1. Self-Attention VS Multi-Head Attention: https://www.geeksforgeeks.org/nlp/self-attention-in-nlp-2/ and https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/
2. LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
3. Transformer-Based Models: https://medium.com/@minh.hoque/a-comprehensive-overview-of-transformer-based-models-encoders-decoders-and-more-e9bc0644a4e5


---

## Difference between PPO, DPO, TPO, GRPO:

<img width="759" height="663" alt="image" src="https://github.com/user-attachments/assets/3930ff59-1573-4f5e-b375-08ccea02a095" />
