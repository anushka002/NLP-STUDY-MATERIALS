1. Self-Attention VS Multi-Head Attention: https://www.geeksforgeeks.org/nlp/self-attention-in-nlp-2/ and https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/
2. LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
3. Transformer-Based Models: https://medium.com/@minh.hoque/a-comprehensive-overview-of-transformer-based-models-encoders-decoders-and-more-e9bc0644a4e5
4. 
